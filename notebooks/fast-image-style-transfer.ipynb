{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-18T07:11:52.110501Z",
     "iopub.status.busy": "2025-01-18T07:11:52.110193Z",
     "iopub.status.idle": "2025-01-18T07:11:59.197563Z",
     "shell.execute_reply": "2025-01-18T07:11:59.196709Z",
     "shell.execute_reply.started": "2025-01-18T07:11:52.110465Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:13:12.397560Z",
     "iopub.status.busy": "2025-01-18T07:13:12.397124Z",
     "iopub.status.idle": "2025-01-18T07:13:12.402988Z",
     "shell.execute_reply": "2025-01-18T07:13:12.401778Z",
     "shell.execute_reply.started": "2025-01-18T07:13:12.397524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN_IMAGE_SIZE = 256\n",
    "DATASET_PATH = \"/kaggle/input/coco-2017-dataset/coco2017/val2017\"\n",
    "NUM_EPOCHS = 1\n",
    "STYLE_IMAGE_PATH = \"/kaggle/input/styles/Styles/udnie.jpg\"\n",
    "BATCH_SIZE = 4 \n",
    "CONTENT_WEIGHT = 17\n",
    "STYLE_WEIGHT = 50\n",
    "TV_WEIGHT = 1e-6 \n",
    "ADAM_LR = 0.001\n",
    "SAVE_MODEL_PATH = \"/kaggle/working/\"\n",
    "SAVE_IMAGE_PATH = \"/kaggle/working/\"\n",
    "SAVE_MODEL_EVERY = 500 # 2,000 Images with batch size 4\n",
    "SEED = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%writefile vgg.py\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision import models, transforms\n",
    "# import utils\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self,vgg_19=True, vgg_path='None'):\n",
    "        super().__init__()\n",
    "        self.vgg_19 = vgg_19\n",
    "        \n",
    "        if self.vgg_19:\n",
    "            vgg_features = models.vgg19(pretrained=True)\n",
    "        else: \n",
    "            vgg_features = models.vgg16(pretrained=True)\n",
    "        self.features = vgg_features.features\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.vgg_19:\n",
    "            layers = {'3': 'relu1_2', '8': 'relu2_2', '17': 'relu3_4', '22': 'relu4_2', '26': 'relu4_4', '35': 'relu5_4'}\n",
    "        else:\n",
    "            layers = {'3': 'relu1_2', '8': 'relu2_2', '15': 'relu3_3', '22': 'relu4_3'}\n",
    "            \n",
    "        features = {}\n",
    "        for name, layer in self.features._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in layers:\n",
    "                features[layers[name]] = x\n",
    "                if (not self.vgg_19 and name == '22'): #or (self.vgg_19 and name == '35'):\n",
    "                    break\n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile transformer.py\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, norm='instance'):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.reflection_pad = nn.ReflectionPad2d(padding)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "        \n",
    "        self.norm = norm\n",
    "        if norm == 'instance':\n",
    "            self.norm_layer = nn.InstanceNorm2d(out_channels)\n",
    "        elif norm == 'batch':\n",
    "            self.norm_layer = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Difference between instance and batch normalization\n",
    "        # Instance Normalization is used for style transfer because it normalizes the activations of the features in the image\n",
    "        # Batch Normalization is used for training the network because it normalizes the activations of the features in the network\n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv(out)\n",
    "        if self.norm == 'none':\n",
    "            return out\n",
    "        out = self.norm_layer(out)\n",
    "        return out\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels=128, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        return out + x\n",
    "    \n",
    "class DeconvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding=1, norm='instance'):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)\n",
    "        \n",
    "        self.norm = norm\n",
    "        if norm == 'instance':\n",
    "            self.norm_layer = nn.InstanceNorm2d(out_channels)\n",
    "        elif norm == 'batch':\n",
    "            self.norm_layer = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.deconv(x)\n",
    "        if self.norm == 'none':\n",
    "            return out\n",
    "        out = self.norm_layer(out)\n",
    "        return out\n",
    "\n",
    "class TransformerNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.ConvBlock = nn.Sequential(\n",
    "            ConvLayer(3, 32, 9, 1),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(32, 64, 3, 2),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(64, 128, 3, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.ResidualBlock = nn.Sequential(\n",
    "            ResidualBlock(128,3),\n",
    "            ResidualBlock(128,3),\n",
    "            ResidualBlock(128,3),\n",
    "            ResidualBlock(128,3),\n",
    "            ResidualBlock(128,3)\n",
    "        )\n",
    "        \n",
    "        self.DeconvBlock = nn.Sequential(\n",
    "            DeconvLayer(128, 64, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            DeconvLayer(64, 32, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(32, 3, 9, 1, norm='none')\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ConvBlock(x)\n",
    "        out = self.ResidualBlock(out)\n",
    "        out = self.DeconvBlock(out)\n",
    "        return out\n",
    "    \n",
    "class TransformerNetworkTanh(TransformerNetwork):\n",
    "    def __init__(self, tanh_multiplier=150, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.tanh_multiplier = tanh_multiplier\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = super().forward(x)\n",
    "        out = self.tanh(out) * self.tanh_multiplier\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile utils.py\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torchvision import transforms, datasets\n",
    "# import cv2\n",
    "# import os\n",
    "\n",
    "def gram_matrix(x):\n",
    "    (b, c, h, w) = x.size()\n",
    "    features = x.view(b, c, h * w)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    # gram = features.bmm(features_t) / (c * h * w)\n",
    "    gram = features.bmm(features, features_t) / (c * h * w)\n",
    "    return gram\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    return img\n",
    "\n",
    "def show_image(img):\n",
    "    # Convert from BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # imshow() only accepts float [0,1] or int [0,255]\n",
    "    img = np.array(img/255).clip(0,1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "def save_image(img, path):\n",
    "    img = img.clip(0, 255)\n",
    "    cv2.imwrite(path, img)\n",
    "    \n",
    "def image_to_tensor(image, max_size=None):\n",
    "    if max_size is None:\n",
    "        itot_t = transforms.Compose([\n",
    "            #transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.mul(255))\n",
    "        ])    \n",
    "    else:\n",
    "        H, W, C = image.shape\n",
    "        image_size = tuple([int((float(max_size) / max([H,W]))*x) for x in [H, W]])\n",
    "        itot_t = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.mul(255))\n",
    "        ])\n",
    "    # Convert image to tensor\n",
    "    tensor = itot_t(image)\n",
    "    # Add the batch_size dimension\n",
    "    tensor = tensor.unsqueeze(dim=0)\n",
    "    return tensor\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    # Add the means\n",
    "    #ttoi_t = transforms.Compose([\n",
    "    #    transforms.Normalize([-103.939, -116.779, -123.68],[1,1,1])])\n",
    "\n",
    "    # Remove the batch_size dimension\n",
    "    tensor = tensor.squeeze()\n",
    "    #img = ttoi_t(tensor)\n",
    "    img = tensor.cpu().numpy()\n",
    "    \n",
    "    # Transpose from [C, H, W] -> [H, W, C]\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "def transfer_color(src, dest):\n",
    "    \"\"\"\n",
    "    Transfer Color using YIQ colorspace. Useful in preserving colors in style transfer.\n",
    "    This method assumes inputs of shape [Height, Width, Channel] in BGR Color Space\n",
    "    \"\"\"\n",
    "    src, dest = src.clip(0,255), dest.clip(0,255)\n",
    "        \n",
    "    # Resize src to dest's size\n",
    "    H,W,_ = src.shape \n",
    "    dest = cv2.resize(dest, dsize=(W, H), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    dest_gray = cv2.cvtColor(dest, cv2.COLOR_BGR2GRAY) #1 Extract the Destination's luminance\n",
    "    src_yiq = cv2.cvtColor(src, cv2.COLOR_BGR2YCrCb)   #2 Convert the Source from BGR to YIQ/YCbCr\n",
    "    src_yiq[...,0] = dest_gray                         #3 Combine Destination's luminance and Source's IQ/CbCr\n",
    "    \n",
    "    return cv2.cvtColor(src_yiq, cv2.COLOR_YCrCb2BGR).clip(0,255)  #4 Convert new image from YIQ back to BGR\n",
    "\n",
    "def plot_loss_hist(c_loss, s_loss, total_loss, title=\"Loss History\"):\n",
    "    x = [i for i in range(len(total_loss))]\n",
    "    plt.figure(figsize=[10, 6])\n",
    "    plt.plot(x, c_loss, label=\"Content Loss\")\n",
    "    plt.plot(x, s_loss, label=\"Style Loss\")\n",
    "    plt.plot(x, total_loss, label=\"Total Loss\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Every 500 iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile data_loader.py\n",
    "# import torch\n",
    "# import os\n",
    "# from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile train.py\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torchvision import datasets, transforms\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# import vgg\n",
    "# import transformer\n",
    "# import utils\n",
    "\n",
    "def train():\n",
    "    # Seeds\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "    # Device\n",
    "    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Dataset and Dataloader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(TRAIN_IMAGE_SIZE),\n",
    "        transforms.CenterCrop(TRAIN_IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Load networks\n",
    "    transformer_network = TransformerNetwork().to(device)\n",
    "    vgg = VGG(vgg_19=False).to(device)\n",
    "    \n",
    "    imagenet_neg_mean = torch.tensor([-103.939, -116.779, -123.68], dtype=torch.float32).reshape(1,3,1,1).to(device)\n",
    "    style_image = load_image(STYLE_IMAGE_PATH)\n",
    "    style_tensor = image_to_tensor(style_image).to(device)\n",
    "    style_tensor = style_tensor.add(imagenet_neg_mean)\n",
    "    B, C, H, W = style_tensor.shape\n",
    "    style_features = vgg(style_tensor.expand([BATCH_SIZE, C, H, W]))\n",
    "    style_gram = {}\n",
    "    for key, value in style_features.items():\n",
    "        style_gram[key] = gram_matrix(value)\n",
    "        \n",
    "    optimizer = optim.Adam(transformer_network.parameters(), lr=ADAM_LR) \n",
    "    \n",
    "    content_loss_history = []\n",
    "    style_loss_history = []\n",
    "    total_loss_history = []\n",
    "    batch_content_loss_sum = 0\n",
    "    batch_style_loss_sum = 0\n",
    "    batch_total_loss_sum = 0\n",
    "    \n",
    "    batch_count = 1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"========Epoch {}/{}========\".format(epoch+1, NUM_EPOCHS))\n",
    "        for content_batch, _ in train_loader:\n",
    "            # Get current batch size in case of odd batch sizes\n",
    "            curr_batch_size = content_batch.shape[0]\n",
    "\n",
    "            # Free-up unneeded cuda memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Zero-out Gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Generate images and get features\n",
    "            content_batch = content_batch[:,[2,1,0]].to(device)\n",
    "            generated_batch = transformer_network(content_batch)\n",
    "            content_features = vgg(content_batch.add(imagenet_neg_mean))\n",
    "            generated_features = vgg(generated_batch.add(imagenet_neg_mean))\n",
    "            \n",
    "            # Content Loss\n",
    "            MSELoss = nn.MSELoss().to(device)\n",
    "            content_loss = CONTENT_WEIGHT * MSELoss(generated_features['relu2_2'], content_features['relu2_2'])            \n",
    "            batch_content_loss_sum += content_loss\n",
    "            \n",
    "            # Style Loss\n",
    "            style_loss = 0\n",
    "            for key, value in generated_features.items():\n",
    "                s_loss = MSELoss(gram_matrix(value), style_gram[key][:curr_batch_size])\n",
    "                style_loss += s_loss\n",
    "            style_loss *= STYLE_WEIGHT\n",
    "            batch_style_loss_sum += style_loss.item()\n",
    "            \n",
    "            # Total Loss\n",
    "            total_loss = content_loss + style_loss\n",
    "            batch_total_loss_sum += total_loss.item()\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (((batch_count-1)%SAVE_MODEL_EVERY == 0) or (batch_count==NUM_EPOCHS*len(train_loader))):\n",
    "                # Print Losses\n",
    "                print(\"========Iteration {}/{}========\".format(batch_count, NUM_EPOCHS*len(train_loader)))\n",
    "                print(\"\\tContent Loss:\\t{:.2f}\".format(batch_content_loss_sum/batch_count))\n",
    "                print(\"\\tStyle Loss:\\t{:.2f}\".format(batch_style_loss_sum/batch_count))\n",
    "                print(\"\\tTotal Loss:\\t{:.2f}\".format(batch_total_loss_sum/batch_count))\n",
    "                print(\"Time elapsed:\\t{} seconds\".format(time.time()-start_time))\n",
    "            \n",
    "                checkpoint_path = SAVE_MODEL_PATH + \"checkpoint_\" + str(batch_count-1) + \".pth\"\n",
    "                torch.save(transformer_network.state_dict(), checkpoint_path)\n",
    "                print(\"Saved transformer_network checkpoint file at {}\".format(checkpoint_path))\n",
    "\n",
    "                sample_tensor = generated_batch[0].clone().detach().unsqueeze(dim=0)\n",
    "                sample_image = tensor_to_image(sample_tensor.clone().detach())\n",
    "                sample_image_path = SAVE_IMAGE_PATH + \"sample0_\" + str(batch_count-1) + \".png\"\n",
    "                save_image(sample_image, sample_image_path)\n",
    "                print(\"Saved sample tranformed image at {}\".format(sample_image_path))\n",
    "                \n",
    "                content_loss_history.append(batch_total_loss_sum/batch_count)\n",
    "                style_loss_history.append(batch_style_loss_sum/batch_count)\n",
    "                total_loss_history.append(batch_total_loss_sum/batch_count)\n",
    "                \n",
    "            batch_count += 1\n",
    "\n",
    "    stop_time = time.time()\n",
    "    # Print loss histories\n",
    "    print(\"Done Training the Transformer Network!\")\n",
    "    print(\"Training Time: {} seconds\".format(stop_time-start_time))\n",
    "    print(\"========Content Loss========\")\n",
    "    print(content_loss_history) \n",
    "    print(\"========Style Loss========\")\n",
    "    print(style_loss_history) \n",
    "    print(\"========Total Loss========\")\n",
    "    print(total_loss_history) \n",
    "    \n",
    "    transformer_network.eval()\n",
    "    transformer_network.cpu()\n",
    "    final_path = SAVE_MODEL_PATH + \"transformer_weight.pth\"\n",
    "    print(\"Saving TransformerNetwork weights at {}\".format(final_path))\n",
    "    torch.save(transformer_network.state_dict(), final_path)\n",
    "    print(\"Done saving final model\")\n",
    "    \n",
    "    return content_loss_history, style_loss_history, total_loss_history\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/kaggle/input/coco-2017-dataset/coco2017/val2017'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m content_loss_history, style_loss_history, total_loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m plot_loss_hist(content_loss_history, style_loss_history, total_loss_history)\n",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Dataset and Dataloader\u001b[39;00m\n\u001b[0;32m     25\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     26\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(TRAIN_IMAGE_SIZE),\n\u001b[0;32m     27\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mCenterCrop(TRAIN_IMAGE_SIZE),\n\u001b[0;32m     28\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     29\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m))\n\u001b[0;32m     30\u001b[0m ])\n\u001b[1;32m---> 32\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Load networks\u001b[39;00m\n",
      "File \u001b[1;32md:\\Vegeta\\Projects\\DL projects\\video style tranfer\\env\\lib\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    321\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m ):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32md:\\Vegeta\\Projects\\DL projects\\video style tranfer\\env\\lib\\site-packages\\torchvision\\datasets\\folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[0;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32md:\\Vegeta\\Projects\\DL projects\\video style tranfer\\env\\lib\\site-packages\\torchvision\\datasets\\folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Vegeta\\Projects\\DL projects\\video style tranfer\\env\\lib\\site-packages\\torchvision\\datasets\\folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/kaggle/input/coco-2017-dataset/coco2017/val2017'"
     ]
    }
   ],
   "source": [
    "content_loss_history, style_loss_history, total_loss_history = train()\n",
    "plot_loss_hist(content_loss_history, style_loss_history, total_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6502779,
     "sourceId": 10503948,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
